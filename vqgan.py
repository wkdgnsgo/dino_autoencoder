import torch
import torch.nn as nn
import yaml
from omegaconf import OmegaConf
from einops import rearrange

# taming-transformersì—ì„œ í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸
from taming.models.vqgan import VQModel
from taming.modules.diffusionmodules.model import ResnetBlock, AttnBlock

# -----------------------------------------------------------------------------
# 1. ì´ì „ ë‹¨ê³„ì—ì„œ ì •ì˜í•œ Cross-Attention ëª¨ë“ˆë“¤
# -----------------------------------------------------------------------------

class CrossAttentionLayer(nn.Module):
    def __init__(self, query_dim, context_dim, n_heads, head_dim):
        super().__init__()
        inner_dim = n_heads * head_dim
        self.scale = head_dim ** -0.5
        self.n_heads = n_heads

        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)
        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)
        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)
        self.to_out = nn.Linear(inner_dim, query_dim)

    def forward(self, x, context):
        q = self.to_q(x)
        k = self.to_k(context)
        v = self.to_v(context)

        # q, k, vë¥¼ head ìˆ˜ë§Œí¼ ë¶„í• 
        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=self.n_heads), (q, k, v))

        # Attention ê³„ì‚°
        sim = torch.einsum('b i d, b j d -> b i j', q, k) * self.scale
        attn = sim.softmax(dim=-1)

        out = torch.einsum('b i j, b j d -> b i d', attn, v)
        out = rearrange(out, '(b h) n d -> b n (h d)', h=self.n_heads)
        return self.to_out(out)

class CrossAttentionDecoderBlock(nn.Module):
    def __init__(self, decoder_dim, context_dim, n_heads=8, head_dim=64):
        super().__init__()
        self.attn = CrossAttentionLayer(decoder_dim, context_dim, n_heads, head_dim)
        self.ffn = nn.Sequential(
            nn.Linear(decoder_dim, decoder_dim * 4),
            nn.GELU(),
            nn.Linear(decoder_dim * 4, decoder_dim)
        )
        self.norm1 = nn.LayerNorm(decoder_dim)
        self.norm2 = nn.LayerNorm(decoder_dim)

    def forward(self, x, context):
        # x shape: (b, h*w, c), context shape: (b, n, d)
        x = x + self.attn(self.norm1(x), context=context)
        x = x + self.ffn(self.norm2(x))
        return x

# -----------------------------------------------------------------------------
# 2. VQGAN ëª¨ë¸ì„ ë¡œë“œí•˜ê³ , Cross-Attentionì„ ì ìš©í•œ ë””ì½”ë”ë¥¼ í¬í•¨í•˜ëŠ” ë©”ì¸ í´ë˜ìŠ¤
# -----------------------------------------------------------------------------

class VQGANDecoderWithCrossAttention(nn.Module):
    def __init__(self, vqgan_config_path, vqgan_ckpt_path, dino_dim=768):
        super().__init__()

        config = OmegaConf.load(vqgan_config_path)
        model = self._load_vqgan(config, vqgan_ckpt_path)
        
        # VQGANì˜ ì£¼ìš” ì»´í¬ë„ŒíŠ¸ ì¶”ì¶œ (ì¸ì½”ë” ì œì™¸)
        self.quantize = model.quantize
        self.quant_conv = model.quant_conv
        self.post_quant_conv = model.post_quant_conv
        
        # ì½”ë“œë¶ ì°¨ì› ê°€ì ¸ì˜¤ê¸°
        codebook_dim = config.model.params.embed_dim

        # DINOv2 íŠ¹ì§•ì„ ì½”ë“œë¶ ì°¨ì›ìœ¼ë¡œ ì •ë ¬í•˜ëŠ” ì„ í˜• ë ˆì´ì–´
        self.linear_align = nn.Linear(dino_dim, codebook_dim)

        # Cross-Attentionì´ ì ìš©ëœ ìƒˆë¡œìš´ ë””ì½”ë” ìƒì„±
        self.decoder = self._build_cross_attention_decoder(model.decoder, dino_dim)
        
        # VQGAN íŒŒë¼ë¯¸í„°ëŠ” í•™ìŠµë˜ì§€ ì•Šë„ë¡ ê³ ì •
        self.quantize.eval()
        self.quant_conv.eval()
        self.post_quant_conv.eval()
        self.decoder.eval()
        for param in self.parameters():
            param.requires_grad = False
        
        # í•™ìŠµì‹œí‚¬ ë¶€ë¶„ë§Œ requires_grad = Trueë¡œ ì„¤ì •
        self.linear_align.requires_grad_(True)
        # ë””ì½”ë”ì˜ cross_attn ë¶€ë¶„ë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ë³€ê²½
        for module in self.decoder.modules():
            if isinstance(module, CrossAttentionDecoderBlock):
                module.requires_grad_(True)
                module.train() # train ëª¨ë“œë¡œ ì„¤ì •
        self.linear_align.train()


    def _load_vqgan(self, config, ckpt_path):
        """YAML ì„¤ì • íŒŒì¼ê³¼ ì²´í¬í¬ì¸íŠ¸ë¡œë¶€í„° VQGAN ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        config = OmegaConf.load(config_path)
        model = VQModel(**config.model.params)
        sd = torch.load(ckpt_path, map_location="cpu")["state_dict"]
        model.load_state_dict(sd, strict=False)
        return model

    def _build_cross_attention_decoder(self, original_decoder, dino_dim):
        """ê¸°ì¡´ ë””ì½”ë” êµ¬ì¡°ì— CrossAttention ë¸”ë¡ì„ ì‚½ì…í•˜ëŠ” í•¨ìˆ˜"""
        new_blocks = nn.ModuleList()
        # original_decoder.bodyëŠ” nn.Sequential
        for module in original_decoder.body:
            new_blocks.append(module)
            # AttnBlock(Self-Attention) ë°”ë¡œ ë’¤ì— Cross-Attention ë¸”ë¡ì„ ì¶”ê°€
            if isinstance(module, AttnBlock):
                # í˜„ì¬ ëª¨ë“ˆì˜ ì±„ë„ ìˆ˜ë¥¼ ê°€ì ¸ì˜´
                decoder_dim = module.in_channels
                # CrossAttention ë¸”ë¡ ì¶”ê°€
                new_blocks.append(CrossAttentionDecoderBlock(decoder_dim, dino_dim))
        
        # ê¸°ì¡´ ë””ì½”ë”ì˜ ë‹¤ë¥¸ ë¶€ë¶„ë“¤ë„ ìœ ì§€
        original_decoder.body = new_blocks
        return original_decoder

    def forward(self, dino_features):
        """
        DINOv2 íŠ¹ì§•ì„ ì…ë ¥ë°›ì•„ ì´ë¯¸ì§€ë¥¼ ë³µì›í•˜ê³  ì–‘ìí™” ì†ì‹¤ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
        dino_features: (batch, num_patches, dino_dim), ì˜ˆ: (B, 256, 768)
        """
        # 1. DINO íŠ¹ì§•ì„ ì½”ë“œë¶ ì°¨ì›ìœ¼ë¡œ ì •ë ¬
        # (B, 256, 768) -> (B, 256, 512)
        aligned_features = self.linear_align(dino_features)

        # 2. VQGANì´ ê¸°ëŒ€í•˜ëŠ” ê³µê°„ì  í˜•íƒœë¡œ ë³€ê²½
        # (B, 256, 512) -> (B, 16, 16, 512) -> (B, 512, 16, 16)
        h = w = int(aligned_features.shape[1] ** 0.5)
        c = aligned_features.shape[2]
        x = rearrange(aligned_features, 'b (h w) c -> b c h w', h=h, w=w)

        # 3. ì–‘ìí™” (Quantization)
        quant_in = self.quant_conv(x)
        z_q, quant_loss, (_, _, indices) = self.quantize(quant_in)

        # 4. Cross-Attentionì´ ì ìš©ëœ ë””ì½”ë”ë¡œ ë³µì›
        quant_out = self.post_quant_conv(z_q)
        
        # ë””ì½”ë” ìˆœíšŒ (Cross-Attention ë¸”ë¡ì€ dino_featuresë¥¼ contextë¡œ ì‚¬ìš©)
        dec_state = quant_out
        for module in self.decoder.body:
            if isinstance(module, CrossAttentionDecoderBlock):
                # (b, c, h, w) -> (b, h*w, c) í˜•íƒœë¡œ ë³€ê²½í•˜ì—¬ ì „ë‹¬
                dec_state_rearranged = rearrange(dec_state, 'b c h w -> b (h w) c')
                # Cross-Attention ìˆ˜í–‰
                attn_out = module(dec_state_rearranged, context=dino_features)
                # ì›ë˜ í˜•íƒœë¡œ ë³µì›
                dec_state = rearrange(attn_out, 'b (h w) c -> b c h w', h=h, w=w)
            else:
                dec_state = module(dec_state)
        
        # ìµœì¢… ì´ë¯¸ì§€ ë³µì›
        reconstructed_image = self.decoder.out(dec_state)

        return reconstructed_image, quant_loss

# -----------------------------------------------------------------------------
# 3. ì‚¬ìš© ì˜ˆì‹œ
# -----------------------------------------------------------------------------

if __name__ == '__main__':
    # ImageNetìœ¼ë¡œ ì‚¬ì „í•™ìŠµëœ VQGAN f16 ëª¨ë¸ì˜ ê²½ë¡œ
    # ì´ íŒŒì¼ë“¤ì€ taming-transformers ë ˆí¬ì§€í† ë¦¬ì—ì„œ ë‹¤ìš´ë¡œë“œ ë°›ì•„ì•¼ í•©ë‹ˆë‹¤.
    # https://omoro.com/models/
    VQGAN_CONFIG_PATH = "./logs/vqgan_imagenet_f16_1024/configs/model.yaml"
    VQGAN_CKPT_PATH = "./logs/vqgan_imagenet_f16_1024/checkpoints/last.ckpt"

    try:
        # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤í™”
        model = VQGANDecoderWithCrossAttention(
            vqgan_config_path=VQGAN_CONFIG_PATH,
            vqgan_ckpt_path=VQGAN_CKPT_PATH,
            dino_dim=768 # DINOv2 Base ëª¨ë¸ì˜ íŠ¹ì§• ì°¨ì›
        )
        print("âœ… VQGANDecoderWithCrossAttention ëª¨ë¸ ìƒì„± ì„±ê³µ!")
        
        # í•™ìŠµì‹œí‚¬ íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"ğŸ§  í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜: {trainable_params:,}")

        # ê°€ìƒ DINOv2 ì¶œë ¥ ìƒì„±
        dummy_dino_features = torch.randn(2, 256, 768) # (Batch, Patches, Dim)

        # ì´ë¯¸ì§€ ë³µì› ì‹¤í–‰
        restored_img = model(dummy_dino_features)

        print("âš¡ï¸ ì´ë¯¸ì§€ ë³µì› ì‹¤í–‰ ì™„ë£Œ!")
        print("DINO ì…ë ¥ Shape:", dummy_dino_features.shape)
        print("ë³µì›ëœ ì´ë¯¸ì§€ Shape:", restored_img.shape) # ì˜ˆ: (2, 3, 256, 256)

    except FileNotFoundError:
        print("âŒ ì—ëŸ¬: VQGAN ì„¤ì • íŒŒì¼ ë˜ëŠ” ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"'{VQGAN_CONFIG_PATH}' ì™€ '{VQGAN_CKPT_PATH}' ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")